{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Котофей\\AppData\\Local\\Temp\\ipykernel_5452\\1599034994.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data = pd.read_csv('data.csv', parse_dates=[0])\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('data.csv', parse_dates=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data['meanVolume']\n",
        "y = (data['density_percent'] >=80).astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    train_size=0.77,\n",
        "                                                    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((483,), (145,))"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred), axis=0) / len(y_true)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m,n \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mshape\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mЧисло объектов в обучающей выборке: \u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mРазмерность объекта: \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "m,n = X_train.shape\n",
        "print(f'Число объектов в обучающей выборке: {m}\\nРазмерность объекта: {n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.w = np.random.randn(n, 1) * 0.001\n",
        "        self.b = np.random.randn() * 0.001\n",
        "        self.report_every = 40\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.005, epochs=40):\n",
        "        self.losses_train = []\n",
        "        self.losses_test = []\n",
        "        \n",
        "        for epoch in range(epochs):            \n",
        "            dw = np.zeros((n, 1))\n",
        "            db = 0\n",
        "            \n",
        "            for i in range(len(X)):\n",
        "                # forward propagation\n",
        "                z = X[i].reshape(1, n).dot(self.w) + self.b\n",
        "                a = sigmoid(z)[0][0]\n",
        "                \n",
        "                # gradient calculation\n",
        "                dw += (a - y[i]) * X[i].reshape(n, 1)\n",
        "                db += (a - y[i])\n",
        "                \n",
        "            dw /= len(X)\n",
        "            db /= len(X)\n",
        "            \n",
        "            # gradient step\n",
        "            self.w = self.w - learning_rate * dw\n",
        "            self.b = self.b - learning_rate * db\n",
        "            \n",
        "            # save loss for plotting\n",
        "            if epoch % self.report_every == 0:\n",
        "                self.losses_train.append(log_loss(y, self.predict(X)))\n",
        "                self.losses_test.append(log_loss(y_test, self.predict(X_test)))\n",
        "        \n",
        "\n",
        "    def train_vec(self, X, y, learning_rate=0.005, epochs=40):\n",
        "        self.losses_train = []\n",
        "        self.losses_test = []\n",
        "        \n",
        "        for epoch in range(epochs):            \n",
        "            Z = X.reshape(m, n).dot(self.w) + self.b\n",
        "            A = sigmoid(Z)\n",
        "            \n",
        "            dw = np.sum(X.reshape(m, n) * (A.reshape(m, 1) - y.reshape(m, 1)), axis=0) / len(X)\n",
        "            db = np.sum((A.reshape(m, 1) - y.reshape(m, 1)), axis=0) / len(X)\n",
        "            \n",
        "            # gradient step\n",
        "            self.w = self.w - learning_rate * dw.reshape(n, 1)\n",
        "            self.b = self.b - learning_rate * db\n",
        "            \n",
        "            # save loss for plotting\n",
        "            if epoch % self.report_every == 0:\n",
        "                self.losses_train.append(log_loss(y, self.predict(X)))\n",
        "                self.losses_test.append(log_loss(y_test, self.predict(X_test)))\n",
        "    \n",
        "\n",
        "    def predict(self, X):        \n",
        "        return np.array([sigmoid(x.reshape(1, n).dot(self.w) + self.b)[0][0] \n",
        "                         for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'tuple' object cannot be interpreted as an integer",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
            "Cell \u001b[1;32mIn[69], line 4\u001b[0m, in \u001b[0;36mLogisticRegression.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandn(n, \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn() \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreport_every \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n",
            "File \u001b[1;32mmtrand.pyx:1270\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mmtrand.pyx:1431\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_common.pyx:636\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "logreg = LogisticRegression()\n",
        "logreg.train_vec(X_train, y_train, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'logreg' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m domain \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(logreg\u001b[39m.\u001b[39mlosses_train)) \u001b[39m*\u001b[39m logreg\u001b[39m.\u001b[39mreport_every\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mplot(domain, logreg\u001b[39m.\u001b[39mlosses_train, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(domain, logreg\u001b[39m.\u001b[39mlosses_test, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'logreg' is not defined"
          ]
        }
      ],
      "source": [
        "domain = np.arange(0, len(logreg.losses_train)) * logreg.report_every\n",
        "plt.plot(domain, logreg.losses_train, label='Train')\n",
        "plt.plot(domain, logreg.losses_test, label='Test')\n",
        "plt.xlabel('Epoch number')\n",
        "plt.ylabel('LogLoss')\n",
        "plt.legend();"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
